{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import os\n",
    "import subprocess\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Ticket detailed description</th>\n",
       "      <th>urgency-Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123456</td>\n",
       "      <td>connection issues with assigned address hi fac...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123457</td>\n",
       "      <td>cannot access hi cannot access fallowing link ...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123458</td>\n",
       "      <td>re address shown valid dear colleagues remarke...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123459</td>\n",
       "      <td>sent tuesday critical alert following alert oc...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123460</td>\n",
       "      <td>code spelling mistake hello should discover fo...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TicketID                        Ticket detailed description  \\\n",
       "0    123456  connection issues with assigned address hi fac...   \n",
       "1    123457  cannot access hi cannot access fallowing link ...   \n",
       "2    123458  re address shown valid dear colleagues remarke...   \n",
       "3    123459  sent tuesday critical alert following alert oc...   \n",
       "4    123460  code spelling mistake hello should discover fo...   \n",
       "\n",
       "  urgency-Priority  \n",
       "0               P1  \n",
       "1               P2  \n",
       "2               P1  \n",
       "3               P2  \n",
       "4               P2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"supportTicketData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"TicketID\" , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_10268\\650793176.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"priority\"] = df[\"priority\"].replace(priority_mapping)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection issues with assigned address hi fac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cannot access hi cannot access fallowing link ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re address shown valid dear colleagues remarke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sent tuesday critical alert following alert oc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>code spelling mistake hello should discover fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  priority\n",
       "0  connection issues with assigned address hi fac...         0\n",
       "1  cannot access hi cannot access fallowing link ...         1\n",
       "2  re address shown valid dear colleagues remarke...         0\n",
       "3  sent tuesday critical alert following alert oc...         1\n",
       "4  code spelling mistake hello should discover fo...         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename({\"Ticket detailed description\":\"description\" , \"urgency-Priority\": \"priority\"} , axis = 1)\n",
    "\n",
    "priority_mapping = {\"P1\":0, \"P2\":1, \"P3\":2}\n",
    "df[\"priority\"] = df[\"priority\"].replace(priority_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\n",
    "# Try loading the model, install if not available\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())  # Convert to lowercase before processing\n",
    "    return \" \".join(token.lemma_ for token in doc if token.is_alpha and not token.is_stop)\n",
    "\n",
    "# Apply preprocessing to the 'description' column in a vectorized manner\n",
    "df[\"description\"] = df[\"description\"].astype(str).map(preprocess_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['description'].values, df['priority'].values, test_size=0.2, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels, vocab=None, max_length=50):\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if vocab is None:\n",
    "            all_words = set(word for text in descriptions for word in text.split())\n",
    "            self.vocab = {word: idx + 1 for idx, word in enumerate(sorted(all_words))}\n",
    "            self.vocab['<PAD>'] = 0\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        token_ids = [self.vocab.get(word, 0) for word in text.split()]\n",
    "        token_ids = token_ids[:self.max_length] + [0] * (self.max_length - len(token_ids))\n",
    "        return token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.encode_text(self.descriptions[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_dataset = TicketDataset(X_train, y_train)\n",
    "val_dataset = TicketDataset(X_val, y_val, vocab=train_dataset.vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(DenseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input for Dense layers\n",
    "        x = x.view(x.size(0), -1).float()\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.flatten_size = None\n",
    "\n",
    "        self.fc = nn.Linear(256 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        \n",
    "        # Convolutional Blocks\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "\n",
    "        if self.flatten_size is None:\n",
    "            self.flatten_size = x.shape[1] * x.shape[2]\n",
    "            self.fc = nn.Linear(self.flatten_size, self.fc.out_features).to(x.device)\n",
    "\n",
    "        # Flatten and Fully Connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model , loss_fn , optimiser ,  train_loader , val_loader , epochs = 10):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for descs, labels in train_loader:\n",
    "            descs, labels = descs.to(DEVICE).long(), labels.to(DEVICE).long()\n",
    "            output = model(descs)\n",
    "            loss = loss_fn(output , labels)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for descs , labels in val_loader:\n",
    "                descs, labels = descs.to(DEVICE).long(), labels.to(DEVICE).long()\n",
    "                outputs = model(descs)\n",
    "                _ , predicted = torch.max(outputs.data , 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = DenseClassifier(50,250,3).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model_dense.parameters() , lr = 0.001)\n",
    "train_model(model_dense , loss_fn , opt ,  train_loader , val_loader , epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNNClassifier(vocab_size=len(train_dataset.vocab), embedding_dim=50, num_classes=3).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model_cnn.parameters() , lr = 0.001)\n",
    "train_model(model_cnn , loss_fn , opt , train_loader , val_loader , EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING & LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_cnn.state_dict() , \"cnn_model\")\n",
    "model_cnn.load_state_dict(torch.load(\"cnn_model\", weights_only=True))\n",
    "# Save model state_dict into a pickle file\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_cnn.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, vocab):\n",
    "    model.eval()\n",
    "    text = preprocess_text(text)\n",
    "    encoded_text = torch.tensor([vocab.get(word, 0) for word in text.split()])\n",
    "    padded_text = torch.cat([encoded_text, torch.zeros(50 - len(encoded_text))]) if len(encoded_text) < 50 else encoded_text[:50]\n",
    "    padded_text = padded_text.unsqueeze(0).long().to(DEVICE)\n",
    "\n",
    "    output_mapping = {0:\"P1\" , 1: \"P2\" , 2:\"P3\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(padded_text)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "    return output_mapping[predicted.cpu().numpy()[0]]\n",
    "\n",
    "predict(model_cnn , \"cannot access hi cannot access fallowing link get blank cannot proceed can you please help with thanks\" , train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
