{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketID</th>\n",
       "      <th>Ticket detailed description</th>\n",
       "      <th>urgency-Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123456</td>\n",
       "      <td>connection issues with assigned address hi fac...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123457</td>\n",
       "      <td>cannot access hi cannot access fallowing link ...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123458</td>\n",
       "      <td>re address shown valid dear colleagues remarke...</td>\n",
       "      <td>P1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123459</td>\n",
       "      <td>sent tuesday critical alert following alert oc...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123460</td>\n",
       "      <td>code spelling mistake hello should discover fo...</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TicketID                        Ticket detailed description urgency-Priority\n",
       "0    123456  connection issues with assigned address hi fac...               P1\n",
       "1    123457  cannot access hi cannot access fallowing link ...               P2\n",
       "2    123458  re address shown valid dear colleagues remarke...               P1\n",
       "3    123459  sent tuesday critical alert following alert oc...               P2\n",
       "4    123460  code spelling mistake hello should discover fo...               P2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"supportTicketData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"TicketID\" , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamal\\AppData\\Local\\Temp\\ipykernel_16508\\650793176.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"priority\"] = df[\"priority\"].replace(priority_mapping)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connection issues with assigned address hi fac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cannot access hi cannot access fallowing link ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re address shown valid dear colleagues remarke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sent tuesday critical alert following alert oc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>code spelling mistake hello should discover fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  priority\n",
       "0  connection issues with assigned address hi fac...         0\n",
       "1  cannot access hi cannot access fallowing link ...         1\n",
       "2  re address shown valid dear colleagues remarke...         0\n",
       "3  sent tuesday critical alert following alert oc...         1\n",
       "4  code spelling mistake hello should discover fo...         1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename({\"Ticket detailed description\":\"description\" , \"urgency-Priority\": \"priority\"} , axis = 1)\n",
    "\n",
    "priority_mapping = {\"P1\":0, \"P2\":1, \"P3\":2}\n",
    "df[\"priority\"] = df[\"priority\"].replace(priority_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  priority\n",
      "0  connection issues with assigned address hi fac...         0\n",
      "1  cannot access hi cannot access fallowing link ...         1\n",
      "2  re address shown valid dear colleagues remarke...         0\n",
      "3  sent tuesday critical alert following alert oc...         1\n",
      "4  code spelling mistake hello should discover fo...         1\n",
      "                                         description  priority\n",
      "0  connection issue assign address hi face connec...         0\n",
      "1  access hi access fallowing link blank proceed ...         1\n",
      "2  address show valid dear colleague remark write...         0\n",
      "3  send tuesday critical alert follow alert occur...         1\n",
      "4  code spelling mistake hello discover code chan...         1\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(df.head())\n",
    "df[\"description\"] = df[\"description\"].apply(preprocess_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['description'].values, df['priority'].values, test_size=0.2, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels, vocab=None, max_length=50):\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if vocab is None:\n",
    "            all_words = set(word for text in descriptions for word in text.split())\n",
    "            self.vocab = {word: idx + 1 for idx, word in enumerate(sorted(all_words))}\n",
    "            self.vocab['<PAD>'] = 0\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        token_ids = [self.vocab.get(word, 0) for word in text.split()]\n",
    "        token_ids = token_ids[:self.max_length] + [0] * (self.max_length - len(token_ids))\n",
    "        return token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.encode_text(self.descriptions[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_dataset = TicketDataset(X_train, y_train)\n",
    "val_dataset = TicketDataset(X_val, y_val, vocab=train_dataset.vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(DenseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input for Dense layers\n",
    "        x = x.view(x.size(0), -1).float()\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.flatten_size = None\n",
    "\n",
    "        self.fc = nn.Linear(256 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        \n",
    "        # Convolutional Blocks\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "\n",
    "        if self.flatten_size is None:\n",
    "            self.flatten_size = x.shape[1] * x.shape[2]\n",
    "            self.fc = nn.Linear(self.flatten_size, self.fc.out_features).to(x.device)\n",
    "\n",
    "        # Flatten and Fully Connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model , loss_fn , optimiser ,  train_loader , val_loader , epochs = 10):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for descs, labels in train_loader:\n",
    "            descs, labels = descs.to(DEVICE).long(), labels.to(DEVICE).long()\n",
    "            output = model(descs)\n",
    "            loss = loss_fn(output , labels)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for descs , labels in val_loader:\n",
    "                descs, labels = descs.to(DEVICE).long(), labels.to(DEVICE).long()\n",
    "                outputs = model(descs)\n",
    "                _ , predicted = torch.max(outputs.data , 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 11684.240381240845\n",
      "Validation Accuracy: 42.10%\n",
      "Epoch 2/20, Loss: 1336.1395165920258\n",
      "Validation Accuracy: 45.15%\n",
      "Epoch 3/20, Loss: 649.2214574217796\n",
      "Validation Accuracy: 41.29%\n",
      "Epoch 4/20, Loss: 532.0528280138969\n",
      "Validation Accuracy: 46.39%\n",
      "Epoch 5/20, Loss: 493.1364198923111\n",
      "Validation Accuracy: 48.61%\n",
      "Epoch 6/20, Loss: 481.618341088295\n",
      "Validation Accuracy: 49.07%\n",
      "Epoch 7/20, Loss: 474.3097249865532\n",
      "Validation Accuracy: 48.46%\n",
      "Epoch 8/20, Loss: 472.3670623898506\n",
      "Validation Accuracy: 48.36%\n",
      "Epoch 9/20, Loss: 474.6995177268982\n",
      "Validation Accuracy: 49.29%\n",
      "Epoch 10/20, Loss: 474.4682620167732\n",
      "Validation Accuracy: 46.21%\n",
      "Epoch 11/20, Loss: 480.94161182641983\n",
      "Validation Accuracy: 48.74%\n",
      "Epoch 12/20, Loss: 483.60386765003204\n",
      "Validation Accuracy: 49.22%\n",
      "Epoch 13/20, Loss: 488.3847697377205\n",
      "Validation Accuracy: 48.69%\n",
      "Epoch 14/20, Loss: 490.5734761953354\n",
      "Validation Accuracy: 42.32%\n",
      "Epoch 15/20, Loss: 493.37685745954514\n",
      "Validation Accuracy: 50.78%\n",
      "Epoch 16/20, Loss: 498.1023304462433\n",
      "Validation Accuracy: 49.67%\n",
      "Epoch 17/20, Loss: 494.42174994945526\n",
      "Validation Accuracy: 49.75%\n",
      "Epoch 18/20, Loss: 501.028835773468\n",
      "Validation Accuracy: 50.45%\n",
      "Epoch 19/20, Loss: 498.3758731484413\n",
      "Validation Accuracy: 50.71%\n",
      "Epoch 20/20, Loss: 493.96280777454376\n",
      "Validation Accuracy: 51.87%\n"
     ]
    }
   ],
   "source": [
    "model_dense = DenseClassifier(50,250,3).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model_dense.parameters() , lr = 0.001)\n",
    "train_model(model_dense , loss_fn , opt ,  train_loader , val_loader , epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 324.30711057782173\n",
      "Validation Accuracy: 71.39%\n",
      "Epoch 2/20, Loss: 255.4980856180191\n",
      "Validation Accuracy: 73.06%\n",
      "Epoch 3/20, Loss: 229.9713936150074\n",
      "Validation Accuracy: 73.41%\n",
      "Epoch 4/20, Loss: 204.12544551491737\n",
      "Validation Accuracy: 72.12%\n",
      "Epoch 5/20, Loss: 168.87557138502598\n",
      "Validation Accuracy: 72.10%\n",
      "Epoch 6/20, Loss: 126.46058061718941\n",
      "Validation Accuracy: 71.79%\n",
      "Epoch 7/20, Loss: 82.29008281044662\n",
      "Validation Accuracy: 70.78%\n",
      "Epoch 8/20, Loss: 45.98248714162037\n",
      "Validation Accuracy: 70.05%\n",
      "Epoch 9/20, Loss: 33.774221881642006\n",
      "Validation Accuracy: 70.93%\n",
      "Epoch 10/20, Loss: 32.7867789005395\n",
      "Validation Accuracy: 69.85%\n",
      "Epoch 11/20, Loss: 24.97391589009203\n",
      "Validation Accuracy: 69.65%\n",
      "Epoch 12/20, Loss: 19.173140569822863\n",
      "Validation Accuracy: 71.79%\n",
      "Epoch 13/20, Loss: 16.098908177751582\n",
      "Validation Accuracy: 71.19%\n",
      "Epoch 14/20, Loss: 22.401313378824852\n",
      "Validation Accuracy: 69.62%\n",
      "Epoch 15/20, Loss: 19.261824028071715\n",
      "Validation Accuracy: 71.21%\n",
      "Epoch 16/20, Loss: 20.409961392637342\n",
      "Validation Accuracy: 71.52%\n",
      "Epoch 17/20, Loss: 17.05506021885958\n",
      "Validation Accuracy: 70.53%\n",
      "Epoch 18/20, Loss: 13.904442541632307\n",
      "Validation Accuracy: 71.11%\n",
      "Epoch 19/20, Loss: 15.02513388756779\n",
      "Validation Accuracy: 71.34%\n",
      "Epoch 20/20, Loss: 14.494348331503716\n",
      "Validation Accuracy: 71.49%\n"
     ]
    }
   ],
   "source": [
    "model_cnn = CNNClassifier(vocab_size=len(train_dataset.vocab), embedding_dim=50, num_classes=3).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model_cnn.parameters() , lr = 0.001)\n",
    "train_model(model_cnn , loss_fn , opt , train_loader , val_loader , EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
